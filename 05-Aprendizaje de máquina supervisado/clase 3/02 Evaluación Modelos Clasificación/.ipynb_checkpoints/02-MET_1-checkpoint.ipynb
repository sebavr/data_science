{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de prueba\n",
    "\n",
    "Vamos a suponer que entrenamos un algoritmo predictivo y compararemos el outcome real de la variable \"y\" versus el outcome predicho por el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.Series([1,0,1,1,0,1,1,1,0,0,1,1,0,0,1,1,0,0,1,0,1,0,0,1,0,0])\n",
    "y_pred = pd.Series([0,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusión\n",
    "\n",
    "Primero vamos a crear una matriz de confusión con la función crosstab (crea una tabla pivoteada con la frecuencia de los elementos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf = pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "# imprima pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1  All\n",
       "True                  \n",
       "0           6   7   13\n",
       "1           5   8   13\n",
       "All        11  15   26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporte de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a armar un reporte de clasificación. Para eso, primero calculamos los falsos/verdaderos positivos/negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete los valores de los arreglos\n",
    "\n",
    "# True Negative\n",
    "TN = pdf[0][0]\n",
    "\n",
    "# True Positive\n",
    "TP = pdf[1][1]\n",
    "\n",
    "# False Negative\n",
    "FN = pdf[1][0]\n",
    "\n",
    "# False Positive\n",
    "FP = pdf[0][1]\n",
    "\n",
    "# Total\n",
    "TOT = TN + TP + FN + FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:8, TN:6, FP:5, FN:7, TOT:26\n"
     ]
    }
   ],
   "source": [
    "print(\"TP:{}, TN:{}, FP:{}, FN:{}, TOT:{}\".format(TP,TN,FP,FN,TOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora armamos las fórmulas de las métricas de precisión, exactitud, sensibilidad, especificidad, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurracy is {} 0.54\n"
     ]
    }
   ],
   "source": [
    "# Accuracy (Precisión)\n",
    "acc =(TP+TN)/TOT\n",
    "print(\"Accurracy is {}\", round(acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is {} 0.62\n"
     ]
    }
   ],
   "source": [
    "# Precision (Exactitud)\n",
    "prec =TP/(FP+TP)\n",
    "print(\"Precision is {}\", round(prec,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall is {} 0.53\n"
     ]
    }
   ],
   "source": [
    "# Recall (Sensibilidad)\n",
    "rec =TP/(TP+FN)\n",
    "print(\"Recall is {}\", round(rec,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specifity is {} 0.55\n"
     ]
    }
   ],
   "source": [
    "# Specifity (Especificidad)\n",
    "spe =TN/(TN+FP)\n",
    "print(\"Specifity is {}\", round(spe,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score is {} 0.57\n"
     ]
    }
   ],
   "source": [
    "# F1-Score = 2 * (precision x recall / (precision + recall))\n",
    "# F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n",
    "f1 =2*(prec*rec)/(prec+rec)\n",
    "print(\"F1 Score is {}\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empleando el package metrics de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: {} 0.54\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score: {}\", round(accuracy_score(y_true,y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: {} 0.53\n"
     ]
    }
   ],
   "source": [
    "# Precision – Accuracy of positive predictions.\n",
    "# Precision = TP/(TP + FP)\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Precision score: {}\", round(precision_score(y_true,y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: {} 0.62\n"
     ]
    }
   ],
   "source": [
    "# Recall (aka sensitivity or true positive rate): Fraction of positives That were correctly identified.\n",
    "# Recall = TP/(TP+FN)\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Recall score: {}\", round(recall_score(y_true,y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: {} 0.57\n"
     ]
    }
   ],
   "source": [
    "# F1 Score (aka F-Score or F-Measure) – A helpful metric for comparing two classifiers. \n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score: {}\", round(f1_score(y_true,y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporte de Clasificación\n",
    "\n",
    "Ahora comparamos con el reporte de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.46      0.50        13\n",
      "           1       0.53      0.62      0.57        13\n",
      "\n",
      "    accuracy                           0.54        26\n",
      "   macro avg       0.54      0.54      0.54        26\n",
      "weighted avg       0.54      0.54      0.54        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report which includes Precision, Recall and F1-Score.\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
